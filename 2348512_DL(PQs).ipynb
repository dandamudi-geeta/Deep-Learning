{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxoycyHSJ19KasbR9fxfqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dandamudi-geeta/Deep-Learning/blob/main/2348512_DL(PQs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Assume we are attempting to train a back-propagation neural network with two hidden layers and a single node in each layer. Let us assume that the activation function is sigmoid function and all the weights and biases are initially set to w=1, b= -0.5, respectively. Compute the forward-pass output for input 0.5. Let the input 0.5 correspond to the target output y =1. Demonstrate the process of learning using Gradient Descent at for 2 iterations to obtain the target 1 for the input 0.5.**"
      ],
      "metadata": {
        "id": "PwuqBOoIU1a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "CYXhav58U4-7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        ""
      ],
      "metadata": {
        "id": "_NGTWvjmU7we"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidDerivative(x):\n",
        "  return x * (1-x)"
      ],
      "metadata": {
        "id": "vAoMhxCMU-Uj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing weights and biases**"
      ],
      "metadata": {
        "id": "pqRfBc88VCr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 1\n",
        "threshold = -0.5"
      ],
      "metadata": {
        "id": "WCn-9hE9VAFm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input and target output\n",
        "x = 0.5\n",
        "targetOutput = 1"
      ],
      "metadata": {
        "id": "N7LkpWPhVI29"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Learning Rate and Iterations\n",
        "learningRate = 0.1\n",
        "iterations = 2"
      ],
      "metadata": {
        "id": "Hg4TFJkGVO-g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for o in range(iterations):\n",
        "\n",
        "    # Forward pass\n",
        "    z1 = w * x + threshold\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    z2 = w * a1 + threshold\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    z3 = w * a2 + threshold\n",
        "    a3 = sigmoid(z3)\n",
        "\n",
        "    z4 = w * a3 + threshold\n",
        "    output = sigmoid(z4)\n",
        "\n",
        "    # Calculating loss\n",
        "    loss = 0.5 * (targetOutput - output) ** 2\n",
        "\n",
        "    # Performing Backpropagation\n",
        "\n",
        "    Output = -(targetOutput - output) * sigmoidDerivative(output)\n",
        "    a3 = Output * w * sigmoidDerivative(a3)\n",
        "    a2 = a3 * w * sigmoidDerivative(a2)\n",
        "    a1 = a2 * w * sigmoidDerivative(a1)\n",
        "\n",
        "    # Updating weights and biases\n",
        "    w -= learningRate * x * a1\n",
        "    threshold -= learningRate * a1\n",
        "\n",
        "    print(f\"Iteration {o + 1}: Output = {output}, Loss = {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO-FMeK0VSBU",
        "outputId": "1bb5439e-6212-48ef-f125-baf7f2b3df2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Output = 0.5, Loss = 0.125\n",
            "Iteration 2: Output = 0.5000810648306058, Loss = 0.12495947087045049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFinal Weights and Biases:\")\n",
        "print(\"Weight (w):\", w)\n",
        "print(\"Bias (b):\", threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPYP6GSBVdWk",
        "outputId": "4baae246-5e8d-4bd4-9501-9b25f40d1f67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Weights and Biases:\n",
            "Weight (w): 1.0001953252666056\n",
            "Bias (b): -0.4996093494667886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Apply L1,L2 and elasticNet regularisation to a single-layered neural network with four input nodes and a bias. Assume the input vector has the value [1,2,2,1] and bias = 0 with no activation function at an output node. The learned parameter vector W = [0.25, 0.25, 0.25, 0.25] returns the expected result of \"2\".**\n",
        "\n",
        "**a) Estimate the total loss using L1 , L2 and Elastic Net regularization separately**\n",
        "\n",
        "**b) Compare the effects of L1 , L2 and Elastic Net regularization techniques**"
      ],
      "metadata": {
        "id": "Cm7mSGVgV-Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputVector = np.array([1,2,2,1])\n",
        "threshold = 0\n",
        "expectedResult = 2\n",
        "weights = np.array([0.25,0.25,0.25,0.25])\n",
        ""
      ],
      "metadata": {
        "id": "BW5qND8JV6bP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss without regularization\n",
        "prediction = threshold + np.dot(inputVector,weights)\n",
        "lossWithoutRegularization = 0.5 * (prediction - expectedResult ) ** 2\n",
        ""
      ],
      "metadata": {
        "id": "jnmrdf7oWVh8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Regularization Parameters\n",
        "\n",
        "l1 = 0.01\n",
        "l2 = 0.01\n",
        "\n",
        "l1Loss = lossWithoutRegularization + l1 * np.sum(np.abs(weights))\n",
        "l2Loss = lossWithoutRegularization + 0.5 * l2 * np.sum(weights**2)\n",
        "\n",
        "elasticNetLoss = lossWithoutRegularization + l1 * np.sum(np.abs(weights))"
      ],
      "metadata": {
        "id": "G-pRO5jxWX-m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loss without Regularization:\", lossWithoutRegularization)\n",
        "print(\"L1 Regularization Loss:\", l1Loss)\n",
        "print(\"L2 Regularization Loss:\", l2Loss)\n",
        "print(\"Elastic Net Regularization Loss:\", elasticNetLoss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwBLrC-HWeTs",
        "outputId": "d191e63c-3841-4f81-85f6-e35ffc80bb37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss without Regularization: 0.125\n",
            "L1 Regularization Loss: 0.135\n",
            "L2 Regularization Loss: 0.12625\n",
            "Elastic Net Regularization Loss: 0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "L1 Regularization Loss and Elastic Net Regularization Loss are higher than the Loss without Regularization. This suggests that the regularization terms are penalizing the model for having non-zero weights. The L2 Regularization Loss is slightly higher than the Loss without Regularization, indicating a penalty for large weights."
      ],
      "metadata": {
        "id": "kdyQMGAtWqDA"
      }
    }
  ]
}